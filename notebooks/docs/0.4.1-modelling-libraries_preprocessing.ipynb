{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41296cc6-9d84-47c5-8a92-2d292f6f3c4a",
   "metadata": {},
   "source": [
    "# Modelling Libraries - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9286e0b8-3c78-4b0f-943c-d219e9840dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to be able to import python package from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2028eed7-b1c3-4c9e-b6a0-00433caa7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `LightGBM` module could not be imported. To enable LightGBM support in Darts, follow the detailed instructions in the installation guide: https://github.com/unit8co/darts/blob/master/INSTALL.md\n",
      "The `Prophet` module could not be imported. To enable Prophet support in Darts, follow the detailed instructions in the installation guide: https://github.com/unit8co/darts/blob/master/INSTALL.md\n",
      "/Users/fred.montet/Library/Caches/pypoetry/virtualenvs/ontime-FpQu8-YN-py3.10/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ontime as on\n",
    "from darts.datasets import EnergyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24da8ab-6a83-4c2f-9ff0-c633d4693a91",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a96d79-0423-4d79-b01d-726193216238",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = EnergyDataset().load()\n",
    "ts = ts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bec6b-eedb-4a88-ba68-dbeae5f0644e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c873dd-8643-40cd-895b-fddd7a515c6d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab9b51-6c63-4068-ac53-98790bf55fde",
   "metadata": {},
   "source": [
    "- [x] Normalize\n",
    "- [x] Split train, test, val\n",
    "- [ ] Feature engineering\n",
    "        - add weather for location\n",
    "        - add day of the week, month, year, etc.\n",
    "        - add whatever\n",
    "- [x] Windowing\n",
    "- [x] Windowing - Split (parts to train as X, parts to predict as y)\n",
    "- [x] Windowing - to tf.data.Dataset\n",
    "- [ ] Windowing - to Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1daa085b-81ad-4569-9f78-3c0884bf93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "def normalize(ts: on.TimeSeries, type='minmax', return_transformer=False):\n",
    "    match type:\n",
    "        case 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        case 'zscore':\n",
    "            scaler = StandardScaler()\n",
    "    transformer = Scaler(scaler)\n",
    "    ts_transformed = transformer.fit_transform(ts)\n",
    "    if return_transformer:\n",
    "        return ts_transformed, transformer\n",
    "    else:\n",
    "        return ts_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de144fa1-d419-46ae-9da1-102db4da92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ts: on.TimeSeries, test_split=None, train_split=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    \n",
    "    :param ts: TimeSeries to split\n",
    "    :param test_split: float, int or pd.TimeStamp\n",
    "    :param train_split: float, int or pd.TimeStamp\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_split is not None and test_split is not None:\n",
    "        raise Exception('Only one of those two parameters can be set : train_split, test_split.')\n",
    "\n",
    "    if train_split is None and test_split is None:\n",
    "        test_split = 0.25\n",
    "    \n",
    "    # split ts in subts : train, test\n",
    "    if test_split is not None: \n",
    "        train_set, test_set = ts.split_after(1-test_split)\n",
    "    \n",
    "    if train_split is not None:\n",
    "        train_set, test_set = ts.split_after(train_split)\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a297972-1588-4539-8168-05ec379c794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_n(ts, n, drop_last=True):\n",
    "\n",
    "    # Get DataFrame\n",
    "    df = ts.pd_dataframe()\n",
    "    \n",
    "    # Calculate the total number of splits needed\n",
    "    total_splits = -(-len(df) // n)  # Ceiling division to get the number of parts\n",
    "    \n",
    "    # Initialize a list to hold the DataFrame splits\n",
    "    splits_df = []\n",
    "    \n",
    "    # Loop through the DataFrame and split it\n",
    "    for split in range(total_splits):\n",
    "        start_index = split * n\n",
    "        end_index = start_index + n\n",
    "        # Append the part to the list, using slicing with .iloc\n",
    "        splits_df.append(df.iloc[start_index:end_index])\n",
    "\n",
    "    # If the last dataframe has a different length, then drop it.\n",
    "    if drop_last:\n",
    "        last_df = splits_df[-1]\n",
    "        second_last = splits_df[-2]        \n",
    "        if len(last_df) != len(second_last):\n",
    "            splits_df = splits_df[:-1]\n",
    "\n",
    "    # Change the data sctructure from DataFrame to TimeSeries\n",
    "    return list(map(on.TimeSeries.from_dataframe, splits_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9614843a-70c2-4213-8d03-e2df030236c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inputs_from_targets(ts_list, input_len, target_len):\n",
    "\n",
    "    # Change inner data structure to DataFrame\n",
    "    dfs = [ts.pd_dataframe() for ts in ts_list]\n",
    "\n",
    "    # Create initial arrays\n",
    "    input_series_list = []\n",
    "    target_series_list = []\n",
    "    \n",
    "    # Iterate over each DataFrame in the list\n",
    "    for df in dfs:\n",
    "        # Check if the DataFrame is large enough to accommodate input_len and label_len\n",
    "        if len(df) >= input_len + target_len:\n",
    "            # Get the first input_len items\n",
    "            input_series = df.iloc[:input_len]\n",
    "            input_series_list.append(input_series)\n",
    "            \n",
    "            # Get the last label_len items\n",
    "            target_series = df.iloc[-target_len:]\n",
    "            target_series_list.append(target_series)\n",
    "        else:\n",
    "            raise Exception('input_len + label_len is longer that the total length of the DataFrame')\n",
    "\n",
    "    input_ts_list = list(map(on.TimeSeries.from_dataframe, input_series_list))\n",
    "    target_ts_list = list(map(on.TimeSeries.from_dataframe, target_series_list))\n",
    "    \n",
    "    return input_ts_list, target_ts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "074f7abd-8d8c-4a9e-a0a7-4215f6a88f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(ts_list):\n",
    "    return np.array([ts.pd_dataframe().to_numpy() for ts in ts_list])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b508ee5-7c7e-4793-904e-45a40df354db",
   "metadata": {},
   "source": [
    "### Test with common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b12f07-8a97-403a-a554-89e166574120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred.montet/Library/Caches/pypoetry/virtualenvs/ontime-FpQu8-YN-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:479: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/Users/fred.montet/Library/Caches/pypoetry/virtualenvs/ontime-FpQu8-YN-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:480: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ts_t = normalize(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84301c56-5e2f-4eea-ad98-a7d0b89c039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ts_t, train_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e3a480-390f-446e-ab08-824f95467ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = split_by_n(train, 6)\n",
    "test_list = split_by_n(test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45e871d-ba2b-4de6-93bc-baf9b26104ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split_inputs_from_targets(train_list, 4, 2)\n",
    "X_test, y_test = split_inputs_from_targets(test_list, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0bc351b-9789-4f0c-914d-6e94d160e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = to_numpy(X_train)\n",
    "y_train = to_numpy(y_train)\n",
    "X_test = to_numpy(X_test)\n",
    "y_test = to_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef9e79a-7c69-446b-a31a-cac8ebce99de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4675, 4, 28)\n",
      "(4675, 2, 28)\n",
      "(1168, 4, 28)\n",
      "(1168, 2, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
