{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41296cc6-9d84-47c5-8a92-2d292f6f3c4a",
   "metadata": {},
   "source": [
    "# Module - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9286e0b8-3c78-4b0f-943c-d219e9840dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to be able to import python package from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2028eed7-b1c3-4c9e-b6a0-00433caa7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts.datasets import EnergyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4733b4e6-71a2-42b2-93fd-a5615b84ac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `LightGBM` module could not be imported. To enable LightGBM support in Darts, follow the detailed instructions in the installation guide: https://github.com/unit8co/darts/blob/master/INSTALL.md\n",
      "The `Prophet` module could not be imported. To enable Prophet support in Darts, follow the detailed instructions in the installation guide: https://github.com/unit8co/darts/blob/master/INSTALL.md\n",
      "/Users/fred.montet/Library/Caches/pypoetry/virtualenvs/ontime-FpQu8-YN-py3.10/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import ontime as on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24da8ab-6a83-4c2f-9ff0-c633d4693a91",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a96d79-0423-4d79-b01d-726193216238",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = EnergyDataset().load()\n",
    "ts = ts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bec6b-eedb-4a88-ba68-dbeae5f0644e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c873dd-8643-40cd-895b-fddd7a515c6d",
   "metadata": {},
   "source": [
    "## Tensorflow Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a630af5c-687e-48e2-a6d4-5a8cb1d1ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontime.module import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b508ee5-7c7e-4793-904e-45a40df354db",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b12f07-8a97-403a-a554-89e166574120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred.montet/Library/Caches/pypoetry/virtualenvs/ontime-FpQu8-YN-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:479: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/Users/fred.montet/Library/Caches/pypoetry/virtualenvs/ontime-FpQu8-YN-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:480: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ts_t = preprocessing.common.normalize(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42428ed1-7556-4341-9675-bad6dca0ecac",
   "metadata": {},
   "source": [
    "### Train test split (for time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b67892d-db8c-4f12-93b6-147016da4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preprocessing.common.train_test_split(ts_t, train_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b0e13-04bc-45ee-ab1a-3996fbfd1df2",
   "metadata": {},
   "source": [
    "### Split time series in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500e954a-82d6-4eff-bbdd-0b889c2a10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = preprocessing.common.split_by_length(train, 6)\n",
    "test_list = preprocessing.common.split_by_length(test, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a88496-6b33-4bff-abb7-1d5ff4c81597",
   "metadata": {},
   "source": [
    "### Split in X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7897c44-71ba-4752-86c6-547387245ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocessing.common.split_inputs_from_targets(train_list, 4, 2)\n",
    "X_test, y_test = preprocessing.common.split_inputs_from_targets(test_list, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626370a-e4ba-4421-b40b-d6e7c5787beb",
   "metadata": {},
   "source": [
    "### Transform in generic data type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ab9cfa-289d-4d8e-be40-d5d4247f5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing.common.timeseries_list_to_numpy(X_train)\n",
    "y_train = preprocessing.common.timeseries_list_to_numpy(y_train)\n",
    "X_test = preprocessing.common.timeseries_list_to_numpy(X_test)\n",
    "y_test = preprocessing.common.timeseries_list_to_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0a2843-6d02-4b08-96f8-91712e521bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4675, 4, 28)\n",
      "(4675, 2, 28)\n",
      "(1168, 4, 28)\n",
      "(1168, 2, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0610c9d9-0597-47a2-878a-630189178a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38a2160b-14b7-461b-b04b-4967212d0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.arange(1, 10)\n",
    "labels = features * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40578937-6e42-49f0-85a9-737832f6a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    features, labels, sequence_length=3, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c6c931a-db3d-4430-a0dd-33618e0b01d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b960eb8b-7d1d-46c9-a703-07685b2b98c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -2, -3, -4, -5, -6, -7, -8, -9])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69bb04c3-3f3a-4367-b252-773420adea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1 2 3]] Target: [-1]\n",
      "Input: [[2 3 4]] Target: [-2]\n",
      "Input: [[3 4 5]] Target: [-3]\n",
      "Input: [[4 5 6]] Target: [-4]\n",
      "Input: [[5 6 7]] Target: [-5]\n",
      "Input: [[6 7 8]] Target: [-6]\n",
      "Input: [[7 8 9]] Target: [-7]\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in dataset:\n",
    "  print(\"Input:\", inputs.numpy(), \"Target:\", targets.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0dfbd-be2f-4a3e-b152-f0bab31bb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, target_width, offset, ts, target_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.ts = ts\n",
    "        self.df = ts.pd_dataframe()\n",
    "\n",
    "        # Work out the target column indices.\n",
    "        self.target_columns = target_columns\n",
    "        if target_columns is not None:\n",
    "            self.target_columns_indices = {name: i for i, name in\n",
    "                                           enumerate(target_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(self.df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.target_width = target_width\n",
    "        self.offset = offset\n",
    "\n",
    "        self.total_window_size = input_width + offset\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.target_start = self.total_window_size - self.target_width\n",
    "        self.targets_slice = slice(self.target_start, None)\n",
    "        self.target_indices = np.arange(self.total_window_size)[self.targets_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Target indices: {self.target_indices}',\n",
    "            f'Target column name(s): {self.target_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        targets = features[:, self.targets_slice, :]\n",
    "        if self.target_columns is not None:\n",
    "            targets = tf.stack(\n",
    "                [targets[:, :, self.column_indices[name]] for name in self.target_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        targets.set_shape([None, self.target_width, None])\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "        return ds.map(self.split_window)\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self.make_dataset(self.df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, targets` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the dataset\n",
    "            result = next(iter(self.dataset))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40116e1ba18a9f9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "target_columns = ['generation solar']\n",
    "input_width=24\n",
    "target_width=12\n",
    "\n",
    "train_window = WindowGenerator(\n",
    "    input_width=input_width,\n",
    "    target_width=target_width,\n",
    "    offset=1,\n",
    "    target_columns=target_columns,\n",
    "    ts=train)\n",
    "\n",
    "val_window = WindowGenerator(\n",
    "    input_width=input_width,\n",
    "    target_width=target_width,\n",
    "    offset=1,\n",
    "    target_columns=target_columns,\n",
    "    ts=val)\n",
    "\n",
    "test_window = WindowGenerator(\n",
    "    input_width=input_width,\n",
    "    target_width=target_width,\n",
    "    offset=1,\n",
    "    target_columns=target_columns,\n",
    "    ts=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6199d923fb0f0d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ed8f7f4104611",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_window.dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ffe3c8a6458b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T19:21:29.856125Z",
     "start_time": "2023-11-17T19:21:29.726757Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_window\u001b[49m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39melement_spec\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_window' is not defined"
     ]
    }
   ],
   "source": [
    "test_window.dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796eb272461a90da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': train_window.dataset,\n",
    "    'val': val_window.dataset,\n",
    "    'test': test_window.dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7491a4ce4265f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(6,),\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac7c0d7fd6fb33",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units].\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features].\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features].\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9b7811703c2f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset['train'],\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=dataset['val'],\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32a7badcfabdd6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "performance = model.evaluate(dataset['test'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa95b9083f5877",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
